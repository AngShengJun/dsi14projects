{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3.1.1 Data Cleaning and Data Science Process\n",
    "---\n",
    "Following scraping textual information from reddit posts, we now continue with the data science process of data cleaning, EDA, model building, evaluation and lastly recommendation of business solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# increase default figure and font sizes for easier viewing\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.rcParams['font.size'] = 14\n",
    "\n",
    "# always be stylish\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in google home reddit data\n",
    "df_gh = pd.read_csv('./reddit_gh.csv')\n",
    "df_gh.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns with the more relevant textual data are from subreddit, selftext and title. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a new dataframe with the columns we want ()\n",
    "df_gh1 = pd.DataFrame(df_gh[['subreddit', 'selftext', 'title']])\n",
    "# Shape of dataframe\n",
    "df_gh1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates in selftext column\n",
    "df_gh1.drop_duplicates(subset='selftext',inplace=True)\n",
    "# Shape of dataframe after removing duplicates\n",
    "df_gh1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NAs.\n",
    "df_gh1.dropna(inplace=True)\n",
    "# Shape of dataframe after removing NAs\n",
    "df_gh1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review first 5 rows of dataframe\n",
    "df_gh1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in google pixel reddit data\n",
    "df_gp = pd.read_csv('./reddit_gp.csv')\n",
    "df_gp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a new dataframe with the columns we want ()\n",
    "df_gp1 = pd.DataFrame(df_gp[['subreddit', 'selftext', 'title']])\n",
    "# Shape of dataframe\n",
    "df_gp1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates in selftext column\n",
    "df_gp1.drop_duplicates(subset='selftext',inplace=True)\n",
    "# Shape of dataframe after removing duplicates\n",
    "df_gp1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NAs.\n",
    "df_gp1.dropna(inplace=True)\n",
    "# Shape of dataframe after removing NAs\n",
    "df_gp1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the two dataframes together\n",
    "df = pd.concat([df_gh1,df_gp1])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "# Shape of concatenated dataframe \n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engineer a feature to turn subreddit into a 1/0 column, where 1 indicates googlehome.\n",
    "df['googlehome'] = df['subreddit'].map({'googlehome': 1, 'GooglePixel': 0})\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['googlehome'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the classes is quite balanced. If the class is imblanced, the resulting model is likely to have low predictive accuracy for the infrequent class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic EDA\n",
    "---\n",
    "Building good performing classifers from data with easily separable classes is relatively straightforward but care should be excercised to determine if such data is representative of the actual world environment. In most real world environment, samples from different classes share similar characteristics or are overlapped. This means the boundaries of each class may not be clearly defined as desired. \n",
    "\n",
    "Common practices to address this problem includes:\n",
    "- i) modifying the original data by introducing/removing features which decrease the overlapping region,\n",
    "- ii) adapting algorithms to reduce the negative impact of overlapping features.\n",
    "\n",
    "Through the basic EDA process, we set out to discover what are the prevalent key words among the reddit posts, and their distribution across text data of googlehome and googlepixel reddit posts. With these insights we would then be in better position to fine-tune the subsequent models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df['selftext']\n",
    "text[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "create a document term matrix with:\n",
    "-ngram range 1,2\n",
    "-minimum document appearance for any term = 2\n",
    "-removal of all English stop words\n",
    "'''\n",
    "# Instantiate countvectorizer\n",
    "cvec = CountVectorizer(ngram_range=(1, 2), stop_words='english', min_df=2)\n",
    "matrix = cvec.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with feature names = words\n",
    "tf = pd.DataFrame(cvec.fit_transform(text).toarray(), columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see most common terms\n",
    "tf.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create googlehome text df\n",
    "googlehome_tdf = df[df['googlehome'] ==1]\n",
    "googlehome_tdf.shape\n",
    "googlehome_tdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create googlepixel text df\n",
    "googlepixel_tdf = df[df['googlehome'] == 0]\n",
    "googlepixel_tdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize selftext from googlehome text\n",
    "tf1 = pd.DataFrame(cvec.fit_transform(googlehome_tdf['selftext']).toarray(), columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No.of features fitted\n",
    "tf1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check value counts\n",
    "tf1.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the words `google`, `home` and `google home` have high prevalence in google home reddit posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize selftext from googlehome text\n",
    "tf2 = pd.DataFrame(cvec.fit_transform(googlepixel_tdf['selftext']).toarray(), columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No.of features fitted\n",
    "tf2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check value counts\n",
    "tf2.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words `google`, `phone` and `pixel` have high prevalence in google pixel reddit posts. Are any of these words common among the two reddit groups, and what is their distribution? Let's find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word counts of googlehome text and google pixel text\n",
    "\"\"\"Merge the two dataframes on Outer join, align with both dataframe indexes.\"\"\"\n",
    "word_counts = pd.merge(pd.DataFrame(data=tf1.sum().sort_values(ascending=False)),\\\n",
    "                       pd.DataFrame(data=tf2.sum().sort_values(ascending=False)),\\\n",
    "                       how='outer',left_index=True,right_index=True)\n",
    "print(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "word_counts.columns = ['googlehome', 'googlepixel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the df to ensure correct column naming\n",
    "word_counts.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find sums\n",
    "word_counts['sum'] = word_counts['googlehome'] + word_counts['googlepixel']\n",
    "# sort by most used values\n",
    "word_counts.sort_values(['sum'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum occurrence of words\n",
    "word_counts.sort_values(['sum'], ascending=True).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 20 most used words\n",
    "word_counts.sort_values(['googlehome','googlepixel'], ascending=False).drop('sum', axis=1).head(30).\\\n",
    "plot(kind='bar',figsize=(15,6),fontsize=12,title='Top 20 common words across reddits');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"googlehome total posts: {df_gh1.shape[0]}, word count: {word_counts['googlehome'].sum()}\")\n",
    "print(f\"googlepixel total posts: {df_gp1.shape[0]}, word count: {word_counts['googlepixel'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the word `google` is unexpectedly common with 1731 instances, an interestingly observation is that it is not as prevalent for google pixel posts compared to google home. This could be due to consumers referring the google handphone simply as `pixel`. \n",
    "For (binary) classification problem, the ideal case is perfect separation in features for the classes. However, this is rarely the case in actual world environment. Observe that there are words that are common between `googlehome` and `googlepixel`. \n",
    "We remove words that have two characteristics:\n",
    "- significant overlaps (smallest difference in word count occurrence in both classes)\n",
    "- occur frequently. From the plot above, word count occurrence of more than 200 is determined emohirically to be a suitable  measure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find words with \n",
    "# 1. significant overlaps\n",
    "# 2. measure by gap (difference of word count between googlehome & googlepixel class)\n",
    "# 3. identify the more significant occuring ones by population proportion\n",
    "word_counts['gap'] = abs(word_counts['googlehome'] - word_counts['googlepixel'])\n",
    "# sort by most used values\n",
    "word_counts.sort_values(['gap'], ascending=True).loc[(word_counts['sum']>300) & (word_counts['gap']<50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts.sort_values(['gap'], ascending=True).loc[(word_counts['sum']>200) & (word_counts['gap']<50)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 10 words commonly overlapping words are [`does`, `work`, `time`, `want`, `use`, `using`, `way`, `assistant`, `ve`, `tried`] are the more common words among the two reddits with significant overlap (similar distribution, with word count aprrox. 200). To measure the impact on model predictive accuracy, it is proposed compare a logreg model without removing these key words and another logreg model with these key words removed.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Art\n",
    "---\n",
    "Following the EDA process, we received a request from the marketing department. They wanted to understand the hot topics for google home and google pixel by inference of the keywords from Reddit posts. While word cloud is technically not data science, we make a small detour to help them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab libraries\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "from matplotlib.pyplot import imread\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of all the text from googlehome\n",
    "wa_gh = pd.DataFrame(cvec.fit_transform(googlehome_tdf['selftext']).toarray(), columns=cvec.get_feature_names())\n",
    "all_textgh = []\n",
    "for x in wa_gh:\n",
    "    all_textgh.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of all the text from googlepixel\n",
    "wa_gp = pd.DataFrame(cvec.fit_transform(googlepixel_tdf['selftext']).toarray(), columns=cvec.get_feature_names())\n",
    "all_textgp = []\n",
    "for x in wa_gp:\n",
    "    all_textgp.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format it into str\n",
    "# In Python 3, unicode has been renamed to str.\n",
    "googlehome_text = str(all_textgh)\n",
    "googlepixel_text = str(all_textgp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to generate wordart (wordcloud)\n",
    "def cloud(source_text):\n",
    "    stop_words = [\"ve'\"] + list(STOPWORDS)\n",
    "    wordcloud = WordCloud(max_words=200,\\\n",
    "                          stopwords=stop_words,\\\n",
    "                          background_color=\"black\",\\\n",
    "                          min_font_size=10,\\\n",
    "                          colormap='viridis').generate(source_text)\n",
    "    \n",
    "    plt.figure(figsize=(9,6))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word cloud image for googlehome\n",
    "cloud(googlehome_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word cloud image for googlepixel\n",
    "cloud(googlepixel_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Prep\n",
    "---\n",
    "First split data into train and test set, then fit only on the train set and transform the test set. This is to prevent data leakage from the test set to the train set (i.e. resultant model will be overfitted and not generalize well to new, unseen data). In other words, excercise caution not to inadvertently countvectorize (a transformer) the data first before doing train-test-split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train test split\n",
    "# X is selftext. y is googlehome.\n",
    "X_train,X_test,y_train,y_test = train_test_split(df[['selftext']],df['googlehome'],test_size=0.25,\\\n",
    "                                                 stratify=df['googlehome'],\\\n",
    "                                                 random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equal proportion of classes split across train and test set\n",
    "print(y_train.value_counts())\n",
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lines of text in test set and train set\n",
    "lines_train = X_train.shape[0]\n",
    "lines_test = X_test.shape[0]\n",
    "print(f\"Lines in train set: {lines_train}.\")\n",
    "print(f\"Lines in test set: {lines_test}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate porterstemmer\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to convert a raw selftext to a string of words\n",
    "# The input is a single string (a raw selftext), and \n",
    "# the output is a single string (a preprocessed selftext)\n",
    "\n",
    "def selftext_to_words(raw_selftext):\n",
    "    \n",
    "    # 1. Remove HTML.\n",
    "    selftext_text = BeautifulSoup(raw_selftext).get_text()\n",
    "    \n",
    "    # 2. Remove non-letters.\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", selftext_text)\n",
    "    \n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()\n",
    "    \n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    # a list, so convert the stopwords to a set.\n",
    "    stops = set(stopwords.words('english'))\n",
    "\n",
    "    # 5. Remove stopwords.\n",
    "    meaningful_words = [w for w in words if w not in stops]\n",
    "    \n",
    "    # 5.5 Stemming of words\n",
    "    meaningful_words = [p_stemmer.stem(w) for w in words]\n",
    "    \n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result\n",
    "    return(\" \".join(meaningful_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize an empty list to hold the clean reviews.\n",
    "X_train_clean = []\n",
    "X_test_clean = []\n",
    "\n",
    "#For train set\n",
    "# Instantiate counter.\n",
    "j = 0\n",
    "for text in X_train['selftext']:\n",
    "    \"\"\"Convert text to words, then append to cX_train_clean.\"\"\"\n",
    "    X_train_clean.append(selftext_to_words(text))\n",
    "    \n",
    "    # If the index is divisible by 1000, print a message.\n",
    "    if (j + 1) % 1000 == 0:\n",
    "        print(f'Clean & parse {j + 1} of {lines_train+lines_test}.')\n",
    "    \n",
    "    j += 1\n",
    "\n",
    "# For test set\n",
    "for text in X_test['selftext']:\n",
    "    \"\"\"Convert text to words, then append to cX_train_clean.\"\"\"\n",
    "    X_test_clean.append(selftext_to_words(text))\n",
    "    \n",
    "    # If the index is divisible by 1629, print a message.\n",
    "    if (j + 1) % (lines_train+lines_test) == 0:\n",
    "        print(f'Clean and parse {j + 1} of {lines_train+lines_test}.')\n",
    "    \n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline accuracy\n",
    "---\n",
    "We first derive the baseline accuracy so as to be able to determine if the subsequent models are better than the baseline (null) model (predicting the plurality class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Baseline accuracy is the percentage of the majority class. In this case, the baseline accuracy is 0.506143. \n",
    "This serves as benchmark for measuring model performance (i.e. model accuracy should be higher than this baseline)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a pipeline with two stages\n",
    "# 1.CountVectorizer (transformer)\n",
    "# 2.LogisticRegression (estimator)\n",
    "pipe = Pipeline([('cvec',CountVectorizer()),\\\n",
    "                 ('logreg',LogisticRegression(solver='lbfgs',max_iter=200,random_state=42))\\\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parameters of pipeline object\n",
    "pipe.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pipeline object into GridSearchCV to tune CountVectorizer\n",
    "# Search over the following values of hyperparameters:\n",
    "# Maximum number of features fit (top no. frequent occur words): 2000, 3000, 4000, 5000\n",
    "# Minimum number of documents (collection of text) needed to include token: 2, 3\n",
    "# Maximum number of documents needed to include token: 90%, 95%\n",
    "# Check (individual tokens) and also check (individual tokens and 2-grams).\n",
    "\n",
    "# n-gram: 1 token, 1-gram, or 1 token, 2-gram.\n",
    "pipe_params = {\n",
    "    'cvec__max_features': [2_000,3_000,4_000,5_000],\\\n",
    "    'cvec__min_df': [2,3],\\\n",
    "    'cvec__max_df': [0.9,0.95],\\\n",
    "    'cvec__ngram_range': [(1, 1), (1,2)]\\\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate GridSearchCV.\n",
    "\"\"\"pipe refers to the object to optimize.\"\"\"\n",
    "\"\"\"param_grid refer to parameter values to search.\"\"\"\n",
    "\"\"\"cv refers to number of cross-validate fold.\"\"\"\n",
    "gs = GridSearchCV(pipe,\\\n",
    "                  param_grid=pipe_params,\\\n",
    "                  cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit GridSearch to the cleaned training data.\n",
    "gs.fit(X_train_clean,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results of the grid search\n",
    "# Note the Score is against one-fifth (hold-out set) of train data\n",
    "print(f\"Best parameters: {gs.best_params_}\")\n",
    "print(f\"Best score: {gs.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model as gs_model.\n",
    "# Trained on four-fifth of data.\n",
    "gs_model = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Score model on training set & testing set\n",
    "print(f\"Accuracy on train set: {gs_model.score(X_train_clean, y_train)}\")\n",
    "print(f\"Accuracy on test set: {gs_model.score(X_test_clean, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model accuracy is higher than the baseline accuracy (0.506). However, the model is overfitted with 7% drop in test accuracy compared to train accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix on the first log reg model\n",
    "# Pass in true values, predicted values to confusion matrix\n",
    "# Convert Confusion matrix into dataframe\n",
    "# Positive class (class 1) is googlehome\n",
    "preds = gs.predict(X_test_clean)\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "cm_df = pd.DataFrame(cm,columns=['pred googlepixel','pred googlehome'], index=['Actual googlepixel','Actual googlehome'])\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The positive class (class 1) refers to `googlehome`. False positive means the observation is classified as `googlehome` when it is actually `googlepixel`. \n",
    "False negative means the ovservation is classified as `googlepixel` when it is actually `googlehome`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return nparray as a 1-D array.\n",
    "confusion_matrix(y_test, preds).ravel()\n",
    "# Save TN/FP/FN/TP values.\n",
    "tn, fp, fn, tp = confusion_matrix(y_test,preds).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of metrics for first log reg model\n",
    "spec = tn/(tn+fp)\n",
    "sens = tp/(tp+fn)\n",
    "print(f\"Specificity: {round(spec,4)}\")\n",
    "print(f\"Sensitivity: {round(sens,4)}\")\n",
    "print(f\"Accuracy: {round(gs.score(X_test_clean,y_test),4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Receiver Operating Characteristic curve is a way to visualize the overlap between our positive class and negative class by moving our classification threshold from 0 to 1. The more area under the blue curve, the better separated the class distributions are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To visualize the ROC AUC curve, first\n",
    "# Create a dataframe called pred_df that contains:\n",
    "# 1. The list of true values of our test set.\n",
    "# 2. The list of predicted probabilities based on our model.\n",
    "\n",
    "pred_proba = [i[1] for i in gs.predict_proba(X_test_clean)]\n",
    "\n",
    "pred_df = pd.DataFrame({'true_values': y_test,\n",
    "                        'pred_probs':pred_proba})\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import roc_auc_score.\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC AUC.\n",
    "roc_auc_score(pred_df['true_values'],pred_df['pred_probs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create figure\n",
    "plt.figure(figsize = (10,7))\n",
    "\n",
    "# Create threshold values. (Dashed orange line in plot.)\n",
    "thresholds = np.linspace(0, 1, 200)\n",
    "\n",
    "# Define function to calculate sensitivity. (True positive rate.)\n",
    "def TPR(df, true_col, pred_prob_col, threshold):\n",
    "    true_positive = df[(df[true_col] == 1) & (df[pred_prob_col] >= threshold)].shape[0]\n",
    "    false_negative = df[(df[true_col] == 1) & (df[pred_prob_col] < threshold)].shape[0]\n",
    "    return true_positive / (true_positive + false_negative)\n",
    "    \n",
    "# Define function to calculate 1 - specificity. (False positive rate.)\n",
    "def FPR(df, true_col, pred_prob_col, threshold):\n",
    "    true_negative = df[(df[true_col] == 0) & (df[pred_prob_col] <= threshold)].shape[0]\n",
    "    false_positive = df[(df[true_col] == 0) & (df[pred_prob_col] > threshold)].shape[0]\n",
    "    return 1 - (true_negative / (true_negative + false_positive))\n",
    "    \n",
    "# Calculate sensitivity & 1-specificity for each threshold between 0 and 1.\n",
    "tpr_values = [TPR(pred_df, 'true_values', 'pred_probs', prob) for prob in thresholds]\n",
    "fpr_values = [FPR(pred_df, 'true_values', 'pred_probs', prob) for prob in thresholds]\n",
    "\n",
    "# Plot ROC curve.\n",
    "plt.plot(fpr_values, # False Positive Rate on X-axis\n",
    "         tpr_values, # True Positive Rate on Y-axis\n",
    "         label='ROC Curve')\n",
    "\n",
    "# Plot baseline. (Perfect overlap between the two populations.)\n",
    "plt.plot(np.linspace(0, 1, 200),\n",
    "         np.linspace(0, 1, 200),\n",
    "         label='baseline',\n",
    "         linestyle='--')\n",
    "\n",
    "# Label axes.\n",
    "plt.title(f'ROC Curve with AUC = {round(roc_auc_score(pred_df[\"true_values\"], pred_df[\"pred_probs\"]),3)}', fontsize=22)\n",
    "plt.ylabel('Sensitivity', fontsize=18)\n",
    "plt.xlabel('1 - Specificity', fontsize=18)\n",
    "\n",
    "# Create legend.\n",
    "plt.legend(fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ROC AUC of 1 means the positive and negative populations are perfectly separated and that the model is as good as it can get. The closer the ROC AUC is to 1, the better. (1 is the maximum score.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would be the impact on model accuracy with the removal of the discovered overlapping key words? Let's find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a pipeline, pipe2 with two stages\n",
    "# 1.CountVectorizer (transformer)\n",
    "# 2.LogisticRegression (estimator)\n",
    "# 3.Remove the words ['does', 'work', 'time', 'want', 'use', 'using', 'way', 'assistant', 've', 'tried'] \n",
    "# via CountVectorizer\n",
    "pipe2 = Pipeline([('cvec',CountVectorizer(stop_words=['does', 'work', 'time', 'want', 'use',\\\n",
    "                                                      'using', 'way', 'assistant', 've', 'tried'])),\\\n",
    "                  ('logreg',LogisticRegression(solver='lbfgs',max_iter=200,random_state=42))\\\n",
    "                 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters of pipeline object\n",
    "pipe2.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pipeline object into GridSearchCV to tune CountVectorizer\n",
    "# Search over the following values of hyperparameters:\n",
    "# Maximum number of features fit (top no. frequent occur words): 2000, 3000, 4000, 5000\n",
    "# Minimum number of documents (collection of text) needed to include token: 2, 3\n",
    "# Maximum number of documents needed to include token: 90%, 95%\n",
    "# Check (individual tokens) and also check (individual tokens and 2-grams).\n",
    "\n",
    "# n-gram: 1 token, 1-gram, or 1 token, 2-gram.\n",
    "pipe2_params = {\n",
    "    'cvec__max_features': [2_000,3_000,4_000,5_000],\\\n",
    "    'cvec__min_df': [2,3],\\\n",
    "    'cvec__max_df': [0.9,0.95],\\\n",
    "    'cvec__ngram_range': [(1, 1), (1,2)]\\\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate GridSearchCV.\n",
    "\"\"\"pipe refers to the object to optimize.\"\"\"\n",
    "\"\"\"param_grid refer to parameter values to search.\"\"\"\n",
    "\"\"\"cv refers to number of cross-validate fold.\"\"\"\n",
    "gs2 = GridSearchCV(pipe2,\\\n",
    "                  param_grid=pipe2_params,\\\n",
    "                  cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit GridSearch to the cleaned training data.\n",
    "gs2.fit(X_train_clean,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results of the grid search\n",
    "# Note the Score is against one-fifth (hold-out set) of train data\n",
    "print(f\"Best parameters: {gs2.best_params_}\")\n",
    "print(f\"Best score: {gs2.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model as gs_model.\n",
    "# Trained on four-fifth of data.\n",
    "gs2_model = gs2.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score model on training set & testing set\n",
    "print(f\"Accuracy on train set: {gs2_model.score(X_train_clean, y_train)}\")\n",
    "print(f\"Accuracy on test set: {gs2_model.score(X_test_clean, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The removal of the identified key words has little impact on improving test accuracy score. It would appear the words removed in the second logistic regression model are not the ones that led to misclassifications. We could revisit this issue on the subsequent best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix on the second log reg model\n",
    "# Pass in true values, predicted values to confusion matrix\n",
    "# Convert Confusion matrix into dataframe\n",
    "# Positive class (class 1) is googlehome\n",
    "preds2 = gs2.predict(X_test_clean)\n",
    "cm = confusion_matrix(y_test, preds2)\n",
    "cm_df = pd.DataFrame(cm,columns=['pred googlepixel','pred googlehome'], index=['Actual googlepixel','Actual googlehome'])\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly, the number of False positive/ False negatives increased with the slight decreased in test accuracy, compared to the first log reg model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return nparray as a 1-D array.\n",
    "confusion_matrix(y_test, preds2).ravel()\n",
    "# Save TN/FP/FN/TP values.\n",
    "tn, fp, fn, tp = confusion_matrix(y_test,preds2).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of metrics for first log reg model\n",
    "spec = tn/(tn+fp)\n",
    "sens = tp/(tp+fn)\n",
    "print(f\"Specificity: {round(spec,4)}\")\n",
    "print(f\"Sensitivity: {round(sens,4)}\")\n",
    "print(f\"Accuracy: {round(gs2.score(X_test_clean,y_test),4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To visualize the ROC AUC curve, first\n",
    "# Create a dataframe called pred_df that contains:\n",
    "# 1. The list of true values of our test set.\n",
    "# 2. The list of predicted probabilities based on our model.\n",
    "\n",
    "pred_proba = [i[1] for i in gs2.predict_proba(X_test_clean)]\n",
    "\n",
    "pred_df = pd.DataFrame({'true_values': y_test,\n",
    "                        'pred_probs':pred_proba})\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC AUC.\n",
    "roc_auc_score(pred_df['true_values'],pred_df['pred_probs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create figure\n",
    "plt.figure(figsize = (10,7))\n",
    "\n",
    "# Create threshold values. (Dashed orange line in plot.)\n",
    "thresholds = np.linspace(0, 1, 200)\n",
    "\n",
    "# Define function to calculate sensitivity. (True positive rate.)\n",
    "def TPR(df, true_col, pred_prob_col, threshold):\n",
    "    true_positive = df[(df[true_col] == 1) & (df[pred_prob_col] >= threshold)].shape[0]\n",
    "    false_negative = df[(df[true_col] == 1) & (df[pred_prob_col] < threshold)].shape[0]\n",
    "    return true_positive / (true_positive + false_negative)\n",
    "    \n",
    "# Define function to calculate 1 - specificity. (False positive rate.)\n",
    "def FPR(df, true_col, pred_prob_col, threshold):\n",
    "    true_negative = df[(df[true_col] == 0) & (df[pred_prob_col] <= threshold)].shape[0]\n",
    "    false_positive = df[(df[true_col] == 0) & (df[pred_prob_col] > threshold)].shape[0]\n",
    "    return 1 - (true_negative / (true_negative + false_positive))\n",
    "    \n",
    "# Calculate sensitivity & 1-specificity for each threshold between 0 and 1.\n",
    "tpr_values = [TPR(pred_df, 'true_values', 'pred_probs', prob) for prob in thresholds]\n",
    "fpr_values = [FPR(pred_df, 'true_values', 'pred_probs', prob) for prob in thresholds]\n",
    "\n",
    "# Plot ROC curve.\n",
    "plt.plot(fpr_values, # False Positive Rate on X-axis\n",
    "         tpr_values, # True Positive Rate on Y-axis\n",
    "         label='ROC Curve')\n",
    "\n",
    "# Plot baseline. (Perfect overlap between the two populations.)\n",
    "plt.plot(np.linspace(0, 1, 200),\n",
    "         np.linspace(0, 1, 200),\n",
    "         label='baseline',\n",
    "         linestyle='--')\n",
    "\n",
    "# Label axes.\n",
    "plt.title(f'ROC Curve with AUC = {round(roc_auc_score(pred_df[\"true_values\"], pred_df[\"pred_probs\"]),3)}', fontsize=22)\n",
    "plt.ylabel('Sensitivity', fontsize=18)\n",
    "plt.xlabel('1 - Specificity', fontsize=18)\n",
    "\n",
    "# Create legend.\n",
    "plt.legend(fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that effect in removal of the identified common overlapping words is minimal on roc_auc is minimal (i.e. roc_auc of first and second logistic regression models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a pipeline, p3 with two stages\n",
    "# 1.CountVectorizer (transformer)\n",
    "# 2.Naive Bayes(multinomial) (estimator)\n",
    "pipe3 = Pipeline([('cvec',CountVectorizer()),\\\n",
    "                 ('nb',MultinomialNB())\\\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parameters of pipeline object\n",
    "pipe3.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pipeline object into GridSearchCV to tune CountVectorizer\n",
    "# Search over the following values of hyperparameters:\n",
    "# Maximum number of features fit (top no. frequent occur words): 2000, 3000, 4000, 5000\n",
    "# Minimum number of documents (collection of text) needed to include token: 2, 3\n",
    "# Maximum number of documents needed to include token: 90%, 95%\n",
    "# Check (individual tokens) and also check (individual tokens and 2-grams).\n",
    "\n",
    "# n-gram: 1 token, 1-gram, or 1 token, 2-gram.\n",
    "pipe_params = {\n",
    "    'cvec__max_features': [2_000,3_000,4_000,5_000],\\\n",
    "    'cvec__min_df': [2,3],\\\n",
    "    'cvec__max_df': [0.9,0.95],\\\n",
    "    'cvec__ngram_range': [(1, 1), (1,2)]\\\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate GridSearchCV.\n",
    "\"\"\"pipe refers to the object to optimize.\"\"\"\n",
    "\"\"\"param_grid refer to parameter values to search.\"\"\"\n",
    "\"\"\"cv refers to number of cross-validate fold.\"\"\"\n",
    "gs3 = GridSearchCV(pipe3,\\\n",
    "                  param_grid=pipe_params,\\\n",
    "                  cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit GridSearch to the cleaned training data.\n",
    "gs3.fit(X_train_clean,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results of the grid search\n",
    "# Note the Score is against one-fifth (hold-out set) of train data\n",
    "print(f\"Best parameters: {gs3.best_params_}\")\n",
    "print(f\"Best score: {gs3.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model as gs_model.\n",
    "# Trained on four-fifth of data.\n",
    "gs_model = gs3.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Score model on training set & testing set\n",
    "print(f\"Accuracy on train set: {gs_model.score(X_train_clean, y_train)}\")\n",
    "print(f\"Accuracy on test set: {gs_model.score(X_test_clean, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The naive bayes test accuracy is higher than the train accuracy, so the model doesn't appear to be overfitted on the training data.\n",
    "Compared to the logistic regression models, naive bayes model has:\n",
    "- lower accuracy on train set\n",
    "- higher accuracy on test set.\n",
    "\n",
    "The naive bayes model performs better than the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix on the first naive bayes model\n",
    "# Pass in true values, predicted values to confusion matrix\n",
    "# Convert Confusion matrix into dataframe\n",
    "# Positive class (class 1) is googlehome\n",
    "preds3 = gs3.predict(X_test_clean)\n",
    "cm = confusion_matrix(y_test, preds3)\n",
    "cm_df = pd.DataFrame(cm,columns=['pred googlepixel','pred googlehome'], index=['Actual googlepixel','Actual googlehome'])\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a higher test accuracy, there is a lower number of False positive/ False negatives in test prediction. Almost equal number of false positives and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return nparray as a 1-D array.\n",
    "confusion_matrix(y_test, preds3).ravel()\n",
    "# Save TN/FP/FN/TP values.\n",
    "tn, fp, fn, tp = confusion_matrix(y_test,preds3).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of metrics for first naive bayes model\n",
    "spec = tn/(tn+fp)\n",
    "sens = tp/(tp+fn)\n",
    "print(f\"Specificity: {round(spec,4)}\")\n",
    "print(f\"Sensitivity: {round(sens,4)}\")\n",
    "print(f\"Accuracy: {round(gs3.score(X_test_clean,y_test),4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To visualize the ROC AUC curve, first\n",
    "# Create a dataframe called pred_df that contains:\n",
    "# 1. The list of true values of our test set.\n",
    "# 2. The list of predicted probabilities based on our model.\n",
    "\n",
    "pred_proba = [i[1] for i in gs3.predict_proba(X_test_clean)]\n",
    "\n",
    "pred_df = pd.DataFrame({'true_values': y_test,\n",
    "                        'pred_probs':pred_proba})\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC AUC.\n",
    "roc_auc_score(pred_df['true_values'],pred_df['pred_probs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create figure\n",
    "plt.figure(figsize = (10,7))\n",
    "\n",
    "# Create threshold values. (Dashed blue line in plot.)\n",
    "thresholds = np.linspace(0, 1, 200)\n",
    "\n",
    "# Define function to calculate sensitivity. (True positive rate.)\n",
    "def TPR(df, true_col, pred_prob_col, threshold):\n",
    "    true_positive = df[(df[true_col] == 1) & (df[pred_prob_col] >= threshold)].shape[0]\n",
    "    false_negative = df[(df[true_col] == 1) & (df[pred_prob_col] < threshold)].shape[0]\n",
    "    return true_positive / (true_positive + false_negative)\n",
    "    \n",
    "# Define function to calculate 1 - specificity. (False positive rate.)\n",
    "def FPR(df, true_col, pred_prob_col, threshold):\n",
    "    true_negative = df[(df[true_col] == 0) & (df[pred_prob_col] <= threshold)].shape[0]\n",
    "    false_positive = df[(df[true_col] == 0) & (df[pred_prob_col] > threshold)].shape[0]\n",
    "    return 1 - (true_negative / (true_negative + false_positive))\n",
    "    \n",
    "# Calculate sensitivity & 1-specificity for each threshold between 0 and 1.\n",
    "tpr_values = [TPR(pred_df, 'true_values', 'pred_probs', prob) for prob in thresholds]\n",
    "fpr_values = [FPR(pred_df, 'true_values', 'pred_probs', prob) for prob in thresholds]\n",
    "\n",
    "# Plot ROC curve.\n",
    "plt.plot(fpr_values, # False Positive Rate on X-axis\n",
    "         tpr_values, # True Positive Rate on Y-axis\n",
    "         label='ROC Curve')\n",
    "\n",
    "# Plot baseline. (Perfect overlap between the two populations.)\n",
    "plt.plot(np.linspace(0, 1, 200),\n",
    "         np.linspace(0, 1, 200),\n",
    "         label='baseline',\n",
    "         linestyle='--')\n",
    "\n",
    "# Label axes.\n",
    "plt.title(f'ROC Curve with AUC = {round(roc_auc_score(pred_df[\"true_values\"], pred_df[\"pred_probs\"]),3)}', fontsize=22)\n",
    "plt.ylabel('Sensitivity', fontsize=18)\n",
    "plt.xlabel('1 - Specificity', fontsize=18)\n",
    "\n",
    "# Create legend.\n",
    "plt.legend(fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ? Check w TA how does one go about describing the ROC curve??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance Summary and Review of Production Model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to print model performance metrics and roc auc\n",
    "def scores(model):\n",
    "    print(classification_report(y_test, model.predict(X_test_clean)))\n",
    "    pred_proba = [i[1] for i in model.predict_proba(X_test_clean)]\n",
    "    print(f\"roc_auc: {round(roc_auc_score(pred_df['true_values'],pred_df['pred_probs']),3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary scores for Logistic Regression Model\n",
    "scores(gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary scores for Logistic Regression Model (words removed)\n",
    "# List of words removed\n",
    "# ['does' 'work', 'time', 'want', 'use', 'using', 'way', 'assistant', 've', 'tried']\n",
    "scores(gs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary scores for Naive Bayes model\n",
    "scores(gs3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deeper Look at Classifications of the Naive Bayes Model \n",
    "---\n",
    "For the test accuracy and roc_auc scores, the Naive Bayes is selected as the better performing model. This section examines the features that\n",
    "- helps with negative (googlepixel) and positive (googlehome) classifications,\n",
    "- what could be the features that lead to misclassifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review model coefficients to see which word is helping with negative / positive classifications)\n",
    "# As GridSearchCV' object has no attribute 'feature_log_prob_,\n",
    "# Build separate naive bayes model to enable model coefficient extraction\n",
    "# using best parameters discovered above in gs3\n",
    "\n",
    "# Instantiate our CountVectorizer\n",
    "cv = CountVectorizer(ngram_range=(1,2),max_df=0.9,min_df=3,max_features=3000)\n",
    "\n",
    "# Fit and transform training data\n",
    "X_train_cleancv = cv.fit_transform(X_train_clean)\n",
    "\n",
    "# Transform test data\n",
    "X_test_cleancv = cv.transform(X_test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# Fit model\n",
    "model = nb.fit(X_train_cleancv,y_train)\n",
    "\n",
    "# Generate predictions\n",
    "predictions = nb.predict(X_test_cleancv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#prob for positive class\n",
    "pos_class_prob_sorted = nb.feature_log_prob_[1, :].argsort()\n",
    "#prob for negative class\n",
    "neg_class_prob_sorted = nb.feature_log_prob_[0, :].argsort()\n",
    "#getting the top features \n",
    "pos_top_features = np.take(cvec.get_feature_names(), pos_class_prob_sorted)\n",
    "neg_top_features = np.take(cvec.get_feature_names(), neg_class_prob_sorted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of top 30 words that helps in 'googlehome' (positive) classification\n",
    "print(pos_top_features[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of top 30 words that helps in 'googlepixel' (negative) classification\n",
    "print(neg_top_features[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, the top 20 words that aided in postive/ negative classifications are neither `google home` nor `google pixel`. \n",
    "\n",
    "**Googlehome**: For positive class (googlehome), it could be inferred that the users reviews seems quite balanced. The word `apple` surfaced, possibly as a mention as comparison of `apple` product performance-related topics with google's. Further analysis could be done into contents with word of negative conotation such as `frustrating`, `frustrated`, `fucking` to better understand the technical issues causing users' frustrations. The main issues with IoT devices are interoperability and connectivity, but it remains to be seen if this is the case for the current google home reddit text analysis.\n",
    "\n",
    "**Googlepixel**: For negative class (googlepixel), there are more words (eight) with negative conotations, from `horrible battery` to `dumb` and `bad batch`. Typical smartphone issues range from dismal battery life to poor build quality and these words seem to reinforce this view. The google R&D and production lines would do well to learn from these negative user reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reviewing the misclassified samples**: What could be the features that lead to misclassifications?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pass y_test (pandas series) into dataframe first\n",
    "# in order to use the original selftext indexes for traceability\n",
    "actual = pd.Series(y_test)\n",
    "df_rvw = actual.to_frame()\n",
    "\n",
    "# Create column of predicted classes from Naive Bayes model\n",
    "df_rvw['pred'] = preds3\n",
    "# Include the selftext data\n",
    "df_rvw['selftext'] = X_test_clean\n",
    "\n",
    "# Review the dataframe\n",
    "df_rvw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index of misclassified classes\n",
    "row_ids = df_rvw[df_rvw['googlehome'] != df_rvw['pred']].index\n",
    "row_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create overview of the misclassified \n",
    "for i in row_ids:\n",
    "    print(df_rvw.loc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two samples from each of misclassified samples are examined in further detail in this section.\n",
    "\n",
    "**False Negative**: Predicted as googlepixel post but is actually googlehome post. Potential reason could be due to the presence of words such as `phone` and `updat` that are more common in the context of smartphones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example 1 of false negative classification\n",
    "print(df_rvw['selftext'][603])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2 of false negative classification\n",
    "print(df_rvw['selftext'][525])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**False Positive**: Predicted as googlehome post but is actually googlepixel post. Potential reason could be due to the presence of words that are related to phone and phone calls (google home can make home calls) that lead to misclassification as pixel (smartphone) post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example 1 of false positive classification\n",
    "print(df_rvw['selftext'][1498])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example 2 of false positive classification\n",
    "print(df_rvw['selftext'][1024])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations and Way Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model good to go. Further enhancements could be implemented in version 2. What more can be done is to switch to review rival product posts for more insights.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
